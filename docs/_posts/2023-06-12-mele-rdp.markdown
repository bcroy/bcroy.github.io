---
layout: post
title:  "Mele RDP"
date:   2023-06-12 12:58:00 -0500
categories: RDPGs
---

The paper _Spectral Estimation of Large Stochastic Blockmodels with Discrete Nodal Covariates_, by Angelo Mele, Lingxin Hao, Joshua Cape, and Carey E. Priebe, shows how to incorporate node-level covariates into stochastic block models (SBMs) as represented by random dot product graphs (RDPGs).

To begin, consider an SBM network with a single block, and nodes having an observable binary covariate. Note that an SBM network with a single block is an Erdos-Renyi random graph with parameter $$p$$ governing the probability of an edge between a pair of nodes. Using the RDPG formalism for an SBM, a node $$i$$ in block $$b$$ is represented by a $$d$$-dimensional latent position vector $$X_b$$ associated with the block, such that $$p=\langle X,X \rangle$$. In the typical case of an SBM with multiple blocks, the RDP formulation characterizes each block $$b_i$$ by a latent position vector $$X_{b_i}$$, such that the likelihood of an edge between nodes $$i$$ and $$j$$ is $$p_{ij} = \langle X_{b_i}, X_{b_j} \rangle$$.

In Mele et. al., the likelihood of an edge between nodes $$i$$ and $$j$$ further depends on observable _covariates_ associated with the nodes. Mele et. al. begin with a simple node-level binary covariate -- if a pair of nodes have the same value of the covariate, then the likelihood of an edge is boosted by a function of a parameter $$\beta$$. More formally, $$p_{ij} = h\left(\langle X_i, X_j \rangle + 1_{\mathrm{same}}(\beta)\right)$$. Here, $$h$$ is simply a function that ensures $$p_{ij}$$ will be a valid probability.

To make this more concrete, consider binary node-level covariate labels __M__ and __F__, again with a single block represented by latent position vector $$X$$. In the Mele et. al. setup we have:\\
__MM__: $$p_{ij} = h\left(\langle X,X \rangle + \beta\right)$$\\
__MF__: $$p_{ij} = h\left(\langle X,X \rangle\right)$$\\
__FF__: $$p_{ij} = h\left(\langle X,X \rangle + \beta\right)$$

We now ask whether this formulation of conditionally applying $$\beta$$ can be instead represented as a node-level covariate vector, such that $$p_{ij}$$ can instead be computed as the inner product of node vectors. In other words, can we transform into a new space where the covariate corresponds to an offset of the node embedding? For $$X$$ and $$\beta$$, would like to find vectors $$Y$$, $$a$$ and $$b$$ such that:\\
__MM__: $$\langle X,X \rangle + \beta   = \langle Y+a, Y+a\rangle$$\\
__MF__: $$\langle X,X \rangle = \langle Y+a, Y+b\rangle$$\\
__FF__: $$\langle X,X \rangle + \beta   = \langle Y+b, Y+b\rangle$$

We begin by expanding and combining the first and third equation. To simplify the notation a bit, we dispense with $$\langle Y,Y \rangle$$ and instead simply write as $$YY$$.\\
$$2Ya + aa = 2Yb + bb$$\\
$$2Y(a-b) + aa - bb = 0$$\\
$$2Y(a-b) = \|b\|^2 - \|a\|^2$$

Next, we observe that the first and second (or the third and second) equations can be combined as follows:\\
$$YY + 2Ya + aa = YY + Ya + Yb + ab + \beta$$\\
$$Ya + aa = Yb + ab + \beta$$\\
$$Ya - Yb + aa - ab = \beta$$\\
$$(Y + a)(a - b) = \beta$$

Finally, we can also rewrite the last equation above as:\\
$$Y(a-b) + a(a-b) = \beta$$
which permits substituting as follows:\\
$$\frac{\|b\|^2 - \|a\|^2}{2} + aa - ab = \beta$$\\
$$bb - aa + 2aa - 2ab = 2\beta$$\\
$$bb+aa - 2ab = 2\beta$$\\
$$\|a-b\|^2 = 2\beta$$

Note that these equations provide some constraints on $$Y$$, $$a$$ and $$b$$. For example, let the vector $$c = a - b$$. Then $$\|c\| = \sqrt{2\beta}$$, which tells us the length of the vector between $$a$$ and $$b$$. 

We can now explore how to transform from $$X$$ and $$\beta$$ to $$Y$$, $$a$$ and $$b$$. We'll proceed with an example and explore what happens with different simplifying assumptions.

### Example: $$X$$ is 1-dimensional
Let's consider an example where $$X$$ is one dimensional -- namely, that $$X = \left[q\right]$$ (thus, $$q = \sqrt{p}$$ with $$p$$ as the probability of an edge within the single block SBM.) I'm not quite sure how to formulate this, but when we introduce a second parameter $$\beta$$ which splits the block based on the node covariates, then we have introduced an additional degree of freedom and thus in our transformed space $$\mathrm{dim}(Y) = 2$$.

Let's explore several simplifying assumptions:

#### Simplifying assumption: $$\|a\| = \|b\|$$

Here, we are assuming that the offsets $$a$$ and $$b$$ induced by covariates __M__ and __F__ have the same magnitude. This implies that $$Y(a-b) = 0$$, that is, the vector from $$a$$ to $$b$$ is orthogonal to $$Y$$.

Suppose that $$Y = \left[q,0\right]$$, then if $$a = \left[r,s\right]$$ we must have $$b = \left[r,-s\right]$$. The first coordinate of $$a$$ and $$b$$ must be the same value ($$r$$) so that it yields vector $$a-b$$ orthogonal to $$Y$$. The second coordinate of $$s$$ and $$-s$$ is due to the fact that the norms are the same, i.e. that $$\|a\| = \|b\|$$. Finally, we have that $$\|a-b\|^2 = 2\beta$$, so $$a$$ and $$b$$ cannot be identical if $$\beta > 0$$.

Geometrically, this means that if $$Y =\left[q,0\right]$$ (along the "$$x$$-axis", for example) the vectors $$a$$ and $$b$$ are orthogonal to $$Y$$ (along the "$$y$$-axis"), pointing in opposite directions, and of the same length determined by $$\beta$$.

Now we can solve for $$a$$ and $$b$$ as follows:\\
$$Y(a-b) + a(a-b) = \beta$$\\
$$a(a-b) = \beta$$ _(since $$Y(a-b) = 0$$)_\\
$$\left[r,s\right] \left[0,2s\right]^T = \beta$$\\
$$2s^2 = \beta$$\\
$$s = \sqrt{\frac{\beta}{2}}$$

Now we can solve for $$r$$ -- i.e. the "$$x$$ offset" in $$a$$ and $$b$$. Returning to the original equations, we'll use\\
$$XX + \beta = \left(Y+a\right)\left(Y+a\right)$$
where we had assumed $$X = \left[q\right]$$. Then\\
$$q^2 + \beta = \left[q+r,s\right] \left[q+r,s\right]^T$$\\
$$q^2 + \beta = q^2 + 2qr + r^2 + s^2$$\\
$$\beta = 2qr + r^2 + \frac{\beta}{2}$$\\
$$r^2 + 2qr - \frac{\beta}{2} = 0$$

Using the quadratic formula $$x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$$, we have\\
$$r = \frac{-2q \pm \sqrt{\left(2q\right)^2 + 4\frac{\beta}{2}}}{2}$$\\
$$r = \frac{-2q \pm \sqrt{4q^2 + 4\frac{\beta}{2}}}{2}$$\\
$$r = -q \pm \sqrt{q^2 + \frac{\beta}{2}}$$

As a simple example, suppose that $$q = .5$$ (which in our SBM would lead to edge probability of $$p = q^2 = .25$$.) Then the above equation becomes\\
$$r = -\frac{1}{2} \pm \sqrt{\frac{1}{4} + \frac{\beta}{2}}$$

__I think there are some mistakes in the quadratic solution below__

$$r^2 + r - \frac{\beta}{2} = 0$$ which we can solve using the quadratic formula to obtain\\
$$r = -\frac{1}{2} \pm \frac{\sqrt{1+2\beta}}{2}$$


So, with $$q = .5$$, the cloud of points for __M__ and __F__ covariates would be offset by $$a = \left[\frac{1}{2} \pm \frac{\sqrt{1+2\beta}}{2}, \sqrt{\frac{\beta}{2}}\right]$$ and 
$$b = \left[\frac{1}{2} \pm \frac{\sqrt{1+2\beta}}{2}, -\sqrt{\frac{\beta}{2}}\right]$$

The __next steps__ are to simulate this scenario, with $$X=\left[q\right]$$ and varying $$\beta$$, and confirming that the above calculations yield point clouds centered at the predicted locations. And to __include some pictures__.

Not to lose sight of the bigger picture, there are additional questions:
- How to develop this for > 1 blocks? Then we have $$X_1$$ and $$X_2$$ giving rise to the higher dimensional embeddings $$Y_1$$ and $$Y_2$$, but preserving the covariate offset vectors $$a$$ and $$b$$.
- In the above presentation we assumed we knew $$\beta$$ and $$X$$. In fact during inference we don't but instead follow the Mele et. al. procedure of embedding followed by clustering and the inferring $$\beta$$. Instead in this approach, we would be trying to factor the clusters into vectors $$X_i$$ and $$a$$ and $$b$$. Not sure how tractable this is...
- What motivated this in the first place was to understand something about how $$\beta$$ contributed to the separability of the clusters (and we also wondered about the original separability of the blocks.) Knowing how $$\beta$$ yields offset vectors $$a$$ and $$b$$ (and perhaps more importantly, the relative distance between them) seems like it should be useful in determining separability of the clusters in a way that $$\beta$$ by itself does not. What might be nice would be if we could bypass some of the details above and just focus on the value of $$\|a-b\|$$ though we would still need to know something about the covariance...
- ... 



#### Simplifying assumption: $$b = \vec{0}$$
In the general case derivations above we showed that $$\|a-b\|^2 = 2\beta$$. With the assumption that $$b = \vec{0}$$, then $$a$$ is a vector with squared norm $$\|a\|^2 = 2\beta$$.

Also, using $$(Y + a)(a - b) = \beta$$ from above and substituting $$b = \vec{0}$$ we have that $$Ya + aa = \beta$$ which simplifies to $$Ya = -\beta$$.

From the one dimensional example described above, we suppose that $$Y = \left[q,0\right]$$, and that $$a = \left[r,s\right]$$, and $$b = \left[0,0\right]$$. So, plugging in to $$Ya = -\beta$$ we have that $$qr = -\beta$$, and $$r = -\frac{\beta}{q}$$.

Since $$\|a\|^2 = 2\beta$$, then $$aa = 2\beta$$, implying that $$r^2 + s^2 = 2\beta$$. This simplifies to $$s = \sqrt{2\beta - r^2}$$. Plugging in $$r = -\frac{\beta}{q}$$ we get that\\
$$s = \sqrt{2\beta - \frac{\beta^2}{q^2}}$$\\
$$s = \sqrt{\frac{1}{q^2} \left[2\beta q^2 - \beta^2\right]}$$\\
$$s = \frac{1}{q} \sqrt{2\beta q^2 - \beta^2}$$\\
$$s = \frac{1}{q} \sqrt{\beta\left(2q^2 - \beta\right)}$$

Working again with the above example with $$q=.5$$, we get that $$r=-2\beta$$ and $$s = 2\sqrt{\frac{1}{2}\beta - \beta^2}$$

This yields $$a = \left[-2\beta, 2\sqrt{\frac{1}{2}\beta - \beta^2}\right]$$



 __Note: didn't finish this part... have to double check signs also, maybe a bug...__

## To do
- Number / label the equations
- Visualizations
