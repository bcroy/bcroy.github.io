---
layout: post
title:  "Mele RDPG"
date:   2023-06-12 12:58:00 -0500
categories: RDPGs
---
<script src="/assets/js/mathjax-config.js" defer></script>

_This is work with Al Gorin, part of our informal "reading group" on network analysis, particularly focused on random dot product graphs. We've been following the excellent work coming out of the Johns Hopkins group_

_Note - this is all still under development_

_2023-08-23: Updated to add the "Alternative, minimal derivation..." section_\\
_2023-09-08: Updated to add the "2-block (and beyond)..." section_

## Introduction

The paper [_Spectral Estimation of Large Stochastic Blockmodels with Discrete Nodal Covariates_](https://www.tandfonline.com/doi/full/10.1080/07350015.2022.2139709), by Angelo Mele, Lingxin Hao, Joshua Cape, and Carey E. Priebe, shows how to incorporate node-level covariates into stochastic block models (SBMs) as represented by random dot product graphs (RDPGs).

To begin, consider an SBM network with a single block, and nodes having an observable binary covariate. Note that an SBM network with a single block is an Erdos-Renyi random graph with parameter $$p$$ governing the probability of an edge between a pair of nodes. Using the RDPG formalism for an SBM, a node $$i$$ in block $$b$$ is represented by a $$d$$-dimensional latent position vector $$X_b$$ associated with the block, such that $$p=\langle X,X \rangle$$. In the typical case of an SBM with multiple blocks, the RDPG formulation characterizes each block $$b$$ by a latent position vector $$X_{b}$$. The probability of an edge between nodes $$i$$ and $$j$$, which are members of blocks $$b_i$$ and $$b_j$$ respectively, is thus $$p_{ij} = \langle X_{b_i}, X_{b_j} \rangle$$.

In Mele et. al., the likelihood of an edge between nodes $$i$$ and $$j$$ further depends on observable _covariates_ associated with the nodes. Mele et. al. begin with a simple node-level binary covariate -- if a pair of nodes have the same value of the covariate, then the likelihood of an edge is boosted by a function of a parameter $$\beta$$. More formally, $$p_{ij} = h\left(\langle X_i, X_j \rangle + 1_{\mathrm{same}}(\beta)\right)$$. Here, $$h$$ is simply a function that ensures $$p_{ij}$$ will be a valid probability.

To make this more concrete, consider binary node-level covariate labels __M__ and __F__, again with a single block represented by latent position vector $$X$$. In the Mele et. al. setup we have:

$$\begin{alignat}{3}
\mathsf{MM:} \ & p_{ij} = h\left(\langle X,X \rangle + \beta\right) \label{eqMM}\\
\mathsf{MF:} \ & p_{ij} = h\left(\langle X,X \rangle\right)         \label{eqMF}\\
\mathsf{FF:} \ & p_{ij} = h\left(\langle X,X \rangle + \beta\right) \label{eqFF}\\
\end{alignat}$$

## New idea -- covariates as offsets in embedding space

We now ask whether this formulation of conditionally applying $$\beta$$ can be instead represented as a node-level covariate vector, such that $$p_{ij}$$ can instead be computed as the inner product of node vectors. In some sense this is conceptually simpler -- each node is given a "complete" representation that captures both its latent position and covariate, and a single function (the inner product) is used to determine the likelihood of an edge between a pair of nodes. In short, can we transform into a new space where the covariate corresponds to an offset of the node embedding? For $$X$$ and $$\beta$$, we would like to find vectors $$Y$$, $$a$$ and $$b$$ such that:

$$\begin{alignat}{3}
\mathsf{MM:} \ & \langle X,X \rangle + \beta & = & \langle Y+a, Y+a\rangle \label{eqMM1}\\
\mathsf{MF:} \ & \langle X,X \rangle         & = & \langle Y+a, Y+b\rangle \label{eqMF1}\\
\mathsf{FF:} \ & \langle X,X \rangle + \beta & = & \langle Y+b, Y+b\rangle \label{eqFF1}\\
\end{alignat}$$


Perhaps this is a stretch or the analogy is misleading, but it feels to me that there is something akin to a "[kernel trick](https://en.wikipedia.org/wiki/Kernel_method){:target="_blank"}{:rel="noopener noreferrer"}" going on here. We are proposing to take our node representations in the original space (latent position vector and covariate) and transform to a new space (a new latent position vector) such that the original function corresponds to an inner product in the new space. 

We begin by expanding and combining equations (\ref{eqMM1}) and (\ref{eqFF1}). To simplify the notation a bit we dispense with inner product notation and instead write, for example, $$\langle Y,Y \rangle$$ as $$YY$$.\\
$$2Ya + aa = 2Yb + bb$$\\
$$2Y(a-b) + aa - bb = 0$$\\
$$2Y(a-b) = \|b\|^2 - \|a\|^2$$

Next, we observe that equations (\ref{eqMM1}) and (\ref{eqMF1}) (or equations (\ref{eqFF1} and (\ref{eqMF1})) can be combined as follows:\\
$$YY + 2Ya + aa = YY + Ya + Yb + ab + \beta$$\\
$$Ya + aa = Yb + ab + \beta$$\\
$$Ya - Yb + aa - ab = \beta$$\\
$$(Y + a)(a - b) = \beta$$

Finally, we can also rewrite the last equation above as:\\
$$Y(a-b) + a(a-b) = \beta$$
which permits substituting as follows:\\
$$\frac{\|b\|^2 - \|a\|^2}{2} + aa - ab = \beta$$\\
$$bb - aa + 2aa - 2ab = 2\beta$$\\
$$bb+aa - 2ab = 2\beta$$\\
$$\|a-b\|^2 = 2\beta$$

Note that these equations provide some constraints on $$Y$$, $$a$$ and $$b$$. We can now explore how to transform from $$X$$ and $$\beta$$ to $$Y$$, $$a$$ and $$b$$. We'll proceed with an example, and show how these equations can be used for inference under some simplifying assumptions.


## Example: 1-block SBM as an RDPG

Consider a simple example -- a 1-block stochastic block model with edge probability $$p$$ (better known as an Erdos-Renyi random graph with parameter $$p$$.) We'll follow the Mele et. al. idea described above and include a single observable binary covariate __M__ or __F__ on each node. If a pair of nodes have the same covariate value (i.e. both are __M__ or __F__), the chance of an edge between them is boosted from $$p$$ to $$p + \beta.$$ For simplicity, we'll only consider values of $$p$$ and $$\beta$$ that yield valid probabilities and dispense with a "squashing" function $$h\left(\cdot\right)$$.

Let us now generate a network (a graph) $$G$$ with $$n=1000$$ nodes, $$p=.16$$, binary node attributes assigned randomly with probabilty $$\pi=.5$$, and attribute coefficient $$\beta=.3$$. This corresponds to an RDPG with a single, 1-dimensional latent position $$\nu = \left[.4\right]$$, thus $$G \sim \mathrm{MeleRDP}\left(\nu,\pi,\beta,n\right)$$.

To obtain node embeddings for $$G$$, we compute the eigendecomposition of its $$n \times n$$ (symmetric) adjacency matrix $$A$$ to get $$A = U \Sigma U^T$$. The top 10 eigenvalues are pictured below, but as is evident only the largest two eigenvalues are meaningful, thus we truncate $$\Sigma$$ to obtain $$\Sigma_*$$. 

![Top 10 eigenvalues](/assets/images/evals_10_nu=.4,beta=.3.png)

We compute the two dimensional embedding vectors $$V = U \Sigma_*^\frac{1}{2}$$, shown below. The mean embedding vectors are shown at the center of each cluster.

![Node embeddings](/assets/images/embedding_nu=.4,beta=.3.png)


### Inferring $$\beta$$

We now show how to infer $$\beta$$ given the embedding vectors as the rows of $$V$$ grouped by observable covariates __M__ and __F__. Recall the equation above $$\|a-b\|^2 = 2\beta$$. Although we do not know the vectors $$Y$$, $$a$$ or $$b$$, we can still compute $$\beta$$ as follows. First, note that each node's embedding vector $$V_i$$ should be close to either $$Y+a$$ or $$Y+b$$ depending on its covariate. This should be done more carefully, but we assert that the mean vector $$\overline{V}_{i \in \mathbf{M}}$$ for nodes $$i$$ with covariate __M__ will approach $$Y + a$$, and the mean vector $$\overline{V}_{i \in \mathbf{F}}$$ for nodes $$i$$ with covariate __F__ will approach $$Y + b$$. Also, note that $$(Y+a) - (Y+b) = a-b$$, so we argue that $$\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}$$ approximates $$a-b$$.

Thus, substituting $$\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}$$ for $$a-b$$, we compute an estimate for $$\beta$$ as
\\[\hat{\beta} = \frac{\|\|\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}\|\|^2}{2}\\]

In the example above, the true value is $$\beta=.3$$ and our estimate yields $$\hat{\beta} = 0.2988$$.

### Inferring $$Y$$, $$a$$ and $$b$$

We now wish to find the latent position vector $$Y$$ corresponding to the block embedding without the effect of the covariates. In so doing, we will also find the covariate "offset vectors" $$a$$ and $$b$$.

I am not sure the best approach to doing this. For the case of $$\beta$$ above we derived an expression directly and from first principles. I would like to do the same here -- for example, there might be a way to find the $$\hat{Y}$$, $$\hat{a}$$ and $$\hat{b}$$ given $$\overline{V_\mathbf{M}}$$ and $$\overline{V_\mathbf{F}}$$ that minimize the error on the constraints above. But for now I will take an empirically motivated approach, which seems intuitively reasonable based on the scatter plot above. Namely, we notice that the orientation of the cluster ellipses seems to point along vectors that intersect, which should correspond to $$a$$ and $$b$$ meeting at the endpoint of $$Y$$.

We first compute the covariance matrices $$\mathrm{Cov}\left[V_\mathbf{M}\right]$$ and $$\mathrm{Cov}\left[V_\mathbf{F}\right]$$, and take the eigenvectors to determine the orientation. The eigenvector with the largest eigenvalue for $$V_\mathbf{M}$$ is $$\left[0.40196, -0.91566\right]$$, $$V_\mathbf{F}$$ is $$\left[0.38517, 0.92285\right]$$.

Next, we compute $$\hat{Y}$$ as the intersection point of the two lines that point along each eigenvector and pass through their respective cluster centroids, as shown below.

![Node embeddings and solved Y, a and b](/assets/images/embedding_nu=.4,beta=.3_solved.png)

With this inference process, we obtain the following values:\\
$$\hat{Y} = \left[0.39114398,0.00350569\right]$$,\\
$$\hat{a} = \left[ 0.16885581,-0.38464754\right]$$, and\\
$$\hat{b} = \left[0.16210216,0.38839087\right]$$

These have the property that $$\langle\hat{Y}+\hat{a},\hat{Y}+\hat{a}\rangle = 0.45887$$, $$\langle\hat{Y}+\hat{b},\hat{Y}+\hat{b}\rangle = 0.45966$$ and $$\langle\hat{Y}+\hat{a},\hat{Y}+\hat{b}\rangle = 0.16045$$. The true values are $$\langle X,X \rangle = .16$$ and $$\langle X, X\rangle + \beta = .46$$.

### Alternative, minimal derivation to obtain $$||X||$$
The original equations above related the latent position vector $$X$$ and scalar $$\beta$$ to a higher dimensional space with new latent position vector $$Y$$ and offset vectors $$a$$ and $$b$$. This permits dispensing with the conditional offset, but at the cost of embedding into more dimensions. However, there are redundant degrees of freedom in the derivation above -- in particular, $$a$$ and $$b$$ are unconstrained. Taking the 1-dimensional example and accounting for the number of parameters in the formula
$$p_{ij} = h\left(\langle X_i, X_j \rangle + 1_{\mathrm{same}}(\beta)\right)$$, we have:
- $$X$$: 1 parameter (since dim=1)
- $$\beta$$: 1 parameter
- $$\mathrm{gender}_i$$, $$\mathrm{gender}_j$$: 2 parameters

This yields a total of 4 parameters.

However, in the higher 2-dimensional formulation $$p_{ij} = \langle Y+\rho_i, Y+\rho_j\rangle$$ where $$\rho = a$$ or $$\rho = b$$, depending on the genders. So, this adds up to 6 total parameters -- more than is needed. In fact, if we collapse $$Y$$ and $$\rho$$ appropriately for each node and let $$Z_i = Y+\rho_i$$, then $$p_{ij} = \langle Z_i, Z_j\rangle$$, and we are back to 4 parameters. This is a rough argument for both the equivalence of this alternative representation to the original, but also that we should simplify the derivation.

So let's instead consider one of the covariates as canonical (e.g. __F__) and incorporate into $$Y$$, viewing the other covariate as a shift $$a$$ from $$Y$$, thus eliminating the $$b$$ offset vector. With this in mind, write the original three equations as:

$$\begin{alignat}{3}
\mathsf{MM:} \ & \langle X,X \rangle + \beta & = & \ \langle Y+a, Y+a\rangle    \label{eqMM2}\\
\mathsf{MF:} \ & \langle X,X \rangle         & = & \ \langle Y+a, Y\rangle      \label{eqMF2}\\
\mathsf{FF:} \ & \langle X,X \rangle + \beta & = & \ \langle Y, Y\rangle        \label{eqFF2}\\
\end{alignat}$$

Then equations (\ref{eqMF2}) and (\ref{eqFF2}) can be combined to yield $$YY+Ya + \beta = YY$$, implying $$\beta = -Ya$$. Furthermore, combining equations (\ref{eqMM2}) and (\ref{eqFF2}) gives $$YY + 2Ya + aa = YY$$, which simplifies to $$2Ya + aa = 0$$, implying that $$aa = 2\beta$$, or that $$\beta = \frac{\|a\|^2}{2}$$. We can then rewrite equation (\ref{eqFF2}) as $$XX = YY - \beta$$, or $$\|X\| = \sqrt{\|Y\|^2 - \beta}$$.

Thus, to estimate $$Y$$, $$a$$, $$\beta$$ and $$\|X\|$$ we have:\\
$$\hat{Y} = \overline{V_\mathbf{F}} = \left[0.46336203, -0.20680491\right]$$,\\
$$\hat{a} = \overline{V_\mathbf{M}} - \overline{V_\mathbf{F}} = \left[-0.01059785, 0.4388148\right]$$,\\
$$\hat{\beta} = \frac{\|\hat{a}\|^2}{2} = 0.3005324201786328$$,\\
$$\|\hat{X}\| = \sqrt{\|\hat{Y}\|^2 - \hat{\beta}} = 0.4014190761582977$$

___(note: the actual numbers are not all consistent since I did multiple runs... need to recompute)___

In summary, $$\beta \approx 0.3005$$ and since in our example $$X$$ is 1-dimensional, $$X = \|X\| \approx .4014$$.

## 2-block (and beyond) SBM as an RDPG

Thus far we have explored a geometric perspective of a single block SBM with a single binary covariate, showing how it could be represented as an RDPG with an offset vector for the covariate. However, this is of little practical utility. We now proceed with the geometric development as above for a 2-block SBM, again viewing the covariate as an offset vector.

### An RDPG with a covariate is an RDPG (after unsquashing...)

Recall that a 2-block SBM is represented by a symmetric 2x2 matrix of probabilities, indicating the probability of an edge between a node in block $$i$$ and a node in block $$j$$. Following the working example in Mele et. al., the likelihood of an edge between nodes with the same covariate is increased according to a function of a parameter $$\beta$$. (Since the addition of $$\beta$$ may yield values that are not valid probabilities, a "squashing" function such as a sigmoid guarantees the addition of offset $$\beta$$ is still a valid probability.) With the addition of a binary covariate, the 2x2 SBM matrix of probabilities becomes a 4x4 matrix, as follows:

$$
\left[
\begin{array}{l l l l}
h(\left<X_1,X_1\right> + \beta) & h(\left<X_1,X_1\right>)         & h(\left<X_1,X_2\right> + \beta) & h(\left<X_1,X_2\right>) \\
h(\left<X_1,X_1\right>)         & h(\left<X_1,X_1\right> + \beta) & h(\left<X_1,X_2\right>          & h(\left<X_1,X_2\right> + \beta) \\
h(\left<X_2,X_1\right> + \beta) & h(\left<X_2,X_1\right>)         & h(\left<X_2,X_2\right> + \beta) & h(\left<X_2,X_2\right>) \\
h(\left<X_2,X_1\right>)         & h(\left<X_2,X_1\right> + \beta) & h(\left<X_2,X_2\right>)         & h(\left<X_2,X_2\right> + \beta) \\
\end{array}
\right]
\notag
$$

This replicates figure 43 on page 16 of Mele et. al. Let us rewrite this expression as the elementwise application of the "squashing" function $$h$$ to matrix $$B$$, with 

$$ B =
\left[
\begin{array}{l l l l}
\left<X_1,X_1\right> + \beta) & \left<X_1,X_1\right>         & \left<X_1,X_2\right> + \beta & \left<X_1,X_2\right> \\
\left<X_1,X_1\right>         & \left<X_1,X_1\right> + \beta & \left<X_1,X_2\right>          & \left<X_1,X_2\right> + \beta \\
\left<X_2,X_1\right> + \beta & \left<X_2,X_1\right>         & \left<X_2,X_2\right> + \beta & \left<X_2,X_2\right> \\
\left<X_2,X_1\right>         & \left<X_2,X_1\right> + \beta & \left<X_2,X_2\right>         & \left<X_2,X_2\right> + \beta \\
\end{array}
\right]
\label{eqB}
$$

Now, let 

$$
Q = \begin{bmatrix}
    \vert & \vert & \vert & \vert \\
    X_1   & X_1   & X_2   & X_2   \\
    \vert & \vert & \vert & \vert \\
\end{bmatrix}, \quad
R = \begin{bmatrix}
\beta & 0     & \beta & 0     \\
0     & \beta & 0     & \beta \\
\beta & 0     & \beta & 0     \\
0     & \beta & 0     & \beta \\
\end{bmatrix}
\notag
$$

Then $$B = Q^T Q + R$$.
We can show that $$B$$ is positive semidefinite since it is the sum of two matrices that are themselves positive semidefinite. First, $$Q^T Q$$ is [positive semidefinite since it is the Gram matrix for the column vectors of $$Q$$](https://en.wikipedia.org/wiki/Gram_matrix#Positive-semidefiniteness). $$R$$ is also positive semidefinite, since it consists of repetitions of a single block. Here, we can apply the formula for the [determinant of a block matrix](https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_determinant) 
$$\det\begin{bmatrix}A & B\\ C & D\end{bmatrix} = \det\left(A\right) \det\left(D - CA^{-1}B\right)$$. In this case $$A = B = C = D$$, and thus the overall determinant is 0 implying $$R$$ is positive semidefinite. (Another way to demonstrate this is via some tedious algebra to show that $$\forall x: x^T R x \geq 0$$.)

Thus, $$B$$ is positive semidefinite since it is the [sum of positive-semidefinite matrices](https://en.wikipedia.org/wiki/Definite_matrix#Addition) $$Q$$ and $$R$$. As a result, $$B$$ is in turn the Gram matrix for a set of vectors that can be obtained via the eigendecomposition, which are simply the latent position vectors of a standard RDPG that has incorporated the effect of the covariate.

### Rank of $$B$$
We have shown how an RDPG representation for a 2-block SBM with the addition of a binary nodal covariate can be represented as an RDPG for the corresponding 4-block SBM. However, a benefit of separating the effect of the covariate ($$\beta$$) from the block latent position vectors is the reduced dimensionality of the problem. How does this manifest in the geometric perspective taken here?

Recall the symmetric 4x4 matrix $$B$$ from equation \ref{eqB}, which we rewrote as $$B = Q^T Q + R$$. We showed that $$B$$ is also positive-semidefinite. We know that the rank of $$Q$$ and $$R$$ are both 2, thus $$\mathrm{rank}\left(B\right) \leq 4$$ (by [_subadditivity of rank_](https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Properties).)

However, we can also see that by [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination), subtracting row 4 from row 3 and row 2 from row 1 of matrix $$B$$ yields two identical rows $$\left[\beta, -\beta, \beta, -\beta\right]$$, one of which would then be eliminated, yielding a row of all zeros implying $$B$$ is rank 3. 

### Factorizing $$B$$ using a covariate offset vector
Following the same approach in the 1-block case, we can refactor $$B$$ in terms of block embedding vectors and a common covariate offset vector. We'll assume new block embedding vectors $$Y_1$$ and $$Y_2$$, and an offset vector $$a$$ corresponding to the covariate __M__. We thus rewrite equation \ref{eqB} as 

$$
B = \left[
\begin{array}{l l l l}
\left<Y_1+a,Y_1+a\right> & \left<Y_1+a,Y_1\right> & \left<Y_1+a,Y_2+a\right> & \left<Y_1+a,Y_2\right> \\
\left<Y_1,Y_1+a\right> & \left<Y_1,Y_1\right> & \left<Y_1,Y_2+a\right> & \left<Y_1,Y_2\right> \\
\left<Y_2+a,Y_1+a\right> & \left<Y_2+a,Y_1\right> & \left<Y_2+a,Y_2+a\right> & \left<Y_2+a,Y_2\right> \\
\left<Y_2,Y_1+a\right> & \left<Y_2,Y_1\right> & \left<Y_2,Y_2+a\right> & \left<Y_2,Y_2\right> \\
\end{array}
\right]
\label{eqB2}
$$

Viewed in this way, $$B = (S+T)^T (S+T)$$ where

$$
S = \begin{bmatrix}
    \vert & \vert & \vert & \vert \\
    Y_1   & Y_1   & Y_2   & Y_2   \\
    \vert & \vert & \vert & \vert \\
\end{bmatrix}, \quad
T = \begin{bmatrix}
\vert & \vert & \vert & \vert \\
a     & 0     & a     & 0 \\
\vert & \vert & \vert & \vert \\
\end{bmatrix}
\notag
$$

As noted above, $$B$$ is the Gram matrix for a set of vectors, which would be the columns of $$S+T$$. We now manipulate \ref{eqB2}, taking the same approach as equations \ref{eqMM2}, \ref{eqMF2}, \ref{eqFF2} -- we can equate the cells of this "offset" representation of $$B$$ with their counterparts in the original form in \ref{eqB}. Thus we have

$$
\begin{alignat}{7}
B_{1,1} & = & B_{2,2}: \     & Y_1 Y_1 + 2Y_1 a + aa & = & \ Y_1 Y_1 & \quad \rightarrow \quad & 2Y_1 a + aa = 0 \notag \\
B_{3,3} & = & B_{4,4}: \      & Y_2 Y_2 + 2Y_2 a + aa & = & \ Y_2 Y_2 & \quad \rightarrow \quad & 2Y_2 a + aa = 0 \notag \\
B_{1,2}+\beta & = & B_{2,2}: \ & Y_1 Y_1 + Y_1 a + \beta \ & = & \ Y_1 Y_1 & \quad \rightarrow \quad & Y_1 a + \beta = 0 \notag \\
B_{1,4}+\beta & = & B_{2,4}: \ & Y_1 Y_2 + Y_2 a + \beta \ & = & \ Y_1 Y_2 & \quad \rightarrow \quad & Y_2 a + \beta = 0 \notag \\
\vdots & & & & & \notag\\
\end{alignat}
$$

Using the same substitutions as done previously, we have 
$$
\beta = \frac{\|a\|^2}{2}
$$.
$$Y_1$$ can be estimated as $$\hat{Y_1} = \overline{V_{1,\mathbf{F}}}$$,  $$\hat{Y_2} = \overline{V_{2,\mathbf{F}}}$$. We can estimate $$a$$ as $$\hat{a} = \overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}$$ using either block 1 or block 2 (or averaging the estimates.)

Note that we have largely ignored the effect of the "squashing" function $$h$$. The development above relies entirely on the $$B$$ matrix and treats the graph as an RDPG. However, in practice Mele et. al. must apply $$h^{-1}$$ in order to tease out the effect of $$\beta$$, and the same is true here. 
Below we simulate example 3 from Mele et. al., which is a 2-block SBM with a binary covariate, using the methods developed here.

__Note:__ I want to mention one small point. I argued that a single offset vector $$a$$ should be applied for the binary covariate for both blocks. This is because the total rank of the combined $$B$$ matrix is 3, not 4. Nonetheless, I did some derivations allowing for a different offset vector for each block. Namely, $$a$$ for block 1 and $$b$$ for block 2. Fortunately, it seems that we can conclude that $$\|a\|^2 = 2\beta$$, $$\|b\|^2 = 2\beta$$ and finally that $$\left<a,b\right> = 2\beta$$. For this to be the case, _I think_ it must be that $$a = b$$.

### Example: Simulation and inference for a 2-block SBM with binary covariate


$$\nu_1 = \left[-1.5, -1.0\right], \quad \nu_2 = \left[1.0, 0.5\right], \quad \beta = 1.5 \notag$$

[//]: nu = np.vstack([nu_1,nu_2]).T
[//]: nu # a d x K matrix



[//]: [\hat{\beta} = \frac{\|\|\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}\|\|^2}{2}\\]



## Implications and next steps

[//]: One motivation of this exploration was to understand cluster separability, which depends on $$\beta$$ as well as 


## To do

- Implications & next steps
- can we show why the covariance matrices are oriented to point toward the endpoint of $$Y$$?
- Develop for num blocks > 1
- Number / label the equations
- Further references on RDPGs, SBMs etc?
