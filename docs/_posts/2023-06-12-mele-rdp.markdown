---
layout: post
title:  "Mele RDP"
date:   2023-06-12 12:58:00 -0500
categories: RDPGs
---
_This is work with Al Gorin, part of our informal "reading group" on network analysis, particularly focused on random dot product graphs. We've been following the excellent work coming out of the Johns Hopkins group_

_Note - this is all still under development_

# Introduction

The paper _Spectral Estimation of Large Stochastic Blockmodels with Discrete Nodal Covariates_, by Angelo Mele, Lingxin Hao, Joshua Cape, and Carey E. Priebe, shows how to incorporate node-level covariates into stochastic block models (SBMs) as represented by random dot product graphs (RDPGs).

To begin, consider an SBM network with a single block, and nodes having an observable binary covariate. Note that an SBM network with a single block is an Erdos-Renyi random graph with parameter $$p$$ governing the probability of an edge between a pair of nodes. Using the RDPG formalism for an SBM, a node $$i$$ in block $$b$$ is represented by a $$d$$-dimensional latent position vector $$X_b$$ associated with the block, such that $$p=\langle X,X \rangle$$. In the typical case of an SBM with multiple blocks, the RDP formulation characterizes each block $$b$$ by a latent position vector $$X_{b}$$. The probability of an edge between nodes $$i$$ and $$j$$, which are members of blocks $$b_i$$ and $$b_j$$ respectively, is thus $$p_{ij} = \langle X_{b_i}, X_{b_j} \rangle$$.

In Mele et. al., the likelihood of an edge between nodes $$i$$ and $$j$$ further depends on observable _covariates_ associated with the nodes. Mele et. al. begin with a simple node-level binary covariate -- if a pair of nodes have the same value of the covariate, then the likelihood of an edge is boosted by a function of a parameter $$\beta$$. More formally, $$p_{ij} = h\left(\langle X_i, X_j \rangle + 1_{\mathrm{same}}(\beta)\right)$$. Here, $$h$$ is simply a function that ensures $$p_{ij}$$ will be a valid probability.

To make this more concrete, consider binary node-level covariate labels __M__ and __F__, again with a single block represented by latent position vector $$X$$. In the Mele et. al. setup we have:\\
__MM__: $$p_{ij} = h\left(\langle X,X \rangle + \beta\right)$$\\
__MF__: $$p_{ij} = h\left(\langle X,X \rangle\right)$$\\
__FF__: $$p_{ij} = h\left(\langle X,X \rangle + \beta\right)$$

# New idea -- covariates as offsets in embedding space

We now ask whether this formulation of conditionally applying $$\beta$$ can be instead represented as a node-level covariate vector, such that $$p_{ij}$$ can instead be computed as the inner product of node vectors. In some sense this is conceptually simpler -- each node is given a "complete" representation that captures both its latent position and covariate, and a single function (the inner product) is used to determine the likelihood of an edge between a pair of nodes. In short, can we transform into a new space where the covariate corresponds to an offset of the node embedding? For $$X$$ and $$\beta$$, we would like to find vectors $$Y$$, $$a$$ and $$b$$ such that:\\
__MM__: $$\langle X,X \rangle + \beta   = \langle Y+a, Y+a\rangle$$\\
__MF__: $$\langle X,X \rangle = \langle Y+a, Y+b\rangle$$\\
__FF__: $$\langle X,X \rangle + \beta   = \langle Y+b, Y+b\rangle$$

Perhaps this is a stretch or the analogy is misleading, but it feels to me that there is something akin to a "[kernel trick](https://en.wikipedia.org/wiki/Kernel_method){:target="_blank"}{:rel="noopener noreferrer"}" going on here. We are proposing to take our node representations in the original space (latent position vector and covariate) and transform to a new space (a new latent position vector) such that the original function corresponds to an inner product in the new space. 

We begin by expanding and combining the first and third equation. To simplify the notation a bit we dispense with inner product notation and instead write, for example, $$\langle Y,Y \rangle$$ as $$YY$$.\\
$$2Ya + aa = 2Yb + bb$$\\
$$2Y(a-b) + aa - bb = 0$$\\
$$2Y(a-b) = \|b\|^2 - \|a\|^2$$

Next, we observe that the first and second (or the third and second) equations can be combined as follows:\\
$$YY + 2Ya + aa = YY + Ya + Yb + ab + \beta$$\\
$$Ya + aa = Yb + ab + \beta$$\\
$$Ya - Yb + aa - ab = \beta$$\\
$$(Y + a)(a - b) = \beta$$

Finally, we can also rewrite the last equation above as:\\
$$Y(a-b) + a(a-b) = \beta$$
which permits substituting as follows:\\
$$\frac{\|b\|^2 - \|a\|^2}{2} + aa - ab = \beta$$\\
$$bb - aa + 2aa - 2ab = 2\beta$$\\
$$bb+aa - 2ab = 2\beta$$\\
$$\|a-b\|^2 = 2\beta$$

Note that these equations provide some constraints on $$Y$$, $$a$$ and $$b$$. We can now explore how to transform from $$X$$ and $$\beta$$ to $$Y$$, $$a$$ and $$b$$. We'll proceed with an example, and show how these equations can be used for inference under some simplifying assumptions.


# Example: 1-block SBM as an RDP

Consider a simple example -- a 1-block stochastic block model with edge probability $$p$$ (better known as an Erdos-Renyi random graph with parameter $$p$$.) We'll follow the Mele et. al. idea described above and include a single observable binary covariate __M__ or __F__ on each node. If a pair of nodes have the same covariate value (i.e. both are __M__ or __F__), the chance of an edge between them is boosted from $$p$$ to $$p + \beta.$$ For simplicity, we'll only consider values of $$p$$ and $$\beta$$ that yield valid probabilities and dispense with a "squashing" function $$h\left(\cdot\right)$$.

Let us now generate a network (a graph) $$G$$ with $$n=1000$$ nodes, $$p=.16$$, binary node attributes assigned randomly with probabilty $$\pi=.5$$, and attribute coefficient $$\beta=.3$$. This corresponds to an RDP with a single, 1-dimensional latent position $$\nu = \left[.4\right]$$, thus $$G \sim \mathrm{MeleRDP}\left(\nu,\pi,\beta,n\right)$$.

To obtain node embeddings for $$G$$, we compute the eigendecomposition of its $$n \times n$$ (symmetric) adjacency matrix $$A$$ to get $$A = U \Sigma U^T$$. The top 10 eigenvalues are pictured below, but as is evident only the largest two eigenvalues are meaningful, thus we truncate $$\Sigma$$ to obtain $$\Sigma_*$$. 

![Top 10 eigenvalues](/assets/images/evals_10_nu=.4,beta=.3.png)

We compute the two dimensional embedding vectors $$V = U \Sigma_*^\frac{1}{2}$$, shown below. The mean embedding vectors are shown at the center of each cluster.

![Node embeddings](/assets/images/embedding_nu=.4,beta=.3.png)


# Inferring $$\beta$$

We now show how to infer $$\beta$$ given the embedding vectors as the rows of $$V$$ grouped by observable covariates __M__ and __F__. Recall the equation above $$\|a-b\|^2 = 2\beta$$. Although we do not know the vectors $$Y$$, $$a$$ or $$b$$, we can still compute $$\beta$$ as follows. First, note that each node's embedding vector $$V_i$$ should be close to either $$Y+a$$ or $$Y+b$$ depending on its covariate. This should be done more carefully, but we assert that the mean vector $$\overline{V}_{i \in \mathbf{M}}$$ for nodes $$i$$ with covariate __M__ will approach $$Y + a$$, and the mean vector $$\overline{V}_{i \in \mathbf{F}}$$ for nodes $$i$$ with covariate __F__ will approach $$Y + b$$. Also, note that $$(Y+a) - (Y+b) = a-b$$, so we argue that $$\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}$$ approximates $$a-b$$.

Thus, substituting $$\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}$$ for $$a-b$$, we compute an estimate for $$\beta$$ as
\\[\hat{\beta} = \frac{\|\|\overline{V_\mathbf{M}} - \overline{V_\mathbf{F}}\|\|^2}{2}\\]

In the example above, the true value is $$\beta=.3$$ and our estimate yields $$\hat{\beta} = 0.2988$$.

# Inferring $$Y$$, $$a$$ and $$b$$

We now wish to find the latent position vector $$Y$$ corresponding to the block embedding without the effect of the covariates. In so doing, we will also find the covariate "offset vectors" $$a$$ and $$b$$.

I am not sure the best approach to doing this -- I feel that there might be a way to find the $$\hat{Y}$$, $$\hat{a}$$ and $$\hat{b}$$ given $$\overline{V_\mathbf{M}}$$ and $$\overline{V_\mathbf{F}}$$ that minimize the error on the constraints above. But for now, I'll go through one approach that seems intuitively reasonable based on the scatter plot above. Namely, to notice that the orientation of the cluster ellipses seems to point along vectors that would intersect, which should correspond to $$a$$ and $$b$$ meeting at the endpoint of $$Y$$.

We first compute the covariance matrices $$\mathrm{Cov}\left[V_\mathbf{M}\right]$$ and $$\mathrm{Cov}\left[V_\mathbf{F}\right]$$, and take the eigenvectors to determine the orientation. The eigenvector with the largest eigenvalue for $$V_\mathbf{M}$$ is $$\left[0.40196, -0.91566\right]$$, $$V_\mathbf{F}$$ is $$\left[0.38517, 0.92285\right]$$.

Next, we compute $$\hat{Y}$$ as the intersection point of the two lines that point along each eigenvector and pass through their respective cluster centroids, as shown below.

![Node embeddings and solved Y, a and b](/assets/images/embedding_nu=.4,beta=.3_solved.png)

With this inference process, we obtain the following values:\\
$$\hat{Y} = \left[0.39114398,0.00350569\right]$$,\\
$$\hat{a} = \left[ 0.16885581,-0.38464754\right]$$, and\\
$$\hat{b} = \left[0.16210216,0.38839087\right]$$

These have the property that $$\langle\hat{Y}+\hat{a},\hat{Y}+\hat{a}\rangle = 0.45887$$, $$\langle\hat{Y}+\hat{b},\hat{Y}+\hat{b}\rangle = 0.45966$$ and $$\langle\hat{Y}+\hat{a},\hat{Y}+\hat{b}\rangle = 0.16045$$. The true values are $$\langle X,X \rangle = .16$$ and $$\langle X, X\rangle + \beta = .46$$.


# Implications and next steps

[//]: One motivation of this exploration was to understand cluster separability, which depends on $$\beta$$ as well as 


## To do
- Implications & next steps
- Number / label the equations
- Develop for num blocks > 1
- Further references on RDPs, SBMs etc?
